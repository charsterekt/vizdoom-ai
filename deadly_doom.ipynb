{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is for all intents and purposes a copy of the `basic_doom` scenario file, but with minor modifications to account for loading a different scenario config and different parameters such as availability of different and more game variables. We will also need to tune parameters, the `env` class, and a different reward structure. <br>\n",
    "\n",
    "We will be tackling the Deadly Corridor scenario in this file. We want the doom skill set to max difficulty so the agent learns to defend itself and we will go over reward shaping to incentivize it properly. The goal is to reach the armour at the end of the corridor and kill all 6 demons along the way. We have additional buttons available to us in this case as well. This is the hardest task for the agent to learn. Added additional game variables to the config in order to shape the reward, being `DAMAGE_TAKEN`, `HITCOUNT` and `SELECTED_WEAPON_AMMO` _without_ any comma separation. We will also have 5 copies of the config file with only the difficulty value changing so the agent can progressively get better at learning what to do and increase in difficulty. This will be done through a process called curriculum learning.\n",
    "\n",
    "NOTE: In my case I ended up with a pretty bad model, but I didn't vary any hyperparameters between checkpoints. Keep that in mind when trying it out for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from vizdoom import *  # Game env\n",
    "import random  # Random action sampling\n",
    "import time  # For sleeping+\n",
    "import numpy as np  # Identity matrix and more\n",
    "\n",
    "# We're going to need to define some shapes to get our frame data working with gym\n",
    "# The way we do this is through Spaces\n",
    "# The types we're going to use are Box and Discrete\n",
    "# Box is an array of any shape\n",
    "# Discrete is a set of discrete binary actions\n",
    "from gym import Env  # OpenAI gym base class\n",
    "from gym.spaces import Box, Discrete  # Spaces for gym\n",
    "import cv2  # OpenCV for image processing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing dependencies for training\n",
    "import os  # For file nav\n",
    "# Callback class for RL\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "# We can use this to check our environment's format\n",
    "from stable_baselines3.common import env_checker\n",
    "# PPO for training\n",
    "from stable_baselines3 import PPO\n",
    "# Eval policy for testing models\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Monitor wrapper\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random actions\n",
    "actions = np.identity(7, dtype=np.uint8)\n",
    "actions\n",
    "\n",
    "# Index 0 represents move left\n",
    "# Index 1 represents move right\n",
    "# Index 2 represents shoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Discrete space\n",
    "Discrete(7).sample()  # Returns 0, 1 or 2\n",
    "# We can use these for our action indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example:\n",
    "actions[Discrete(7).sample()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Box space\n",
    "Box(low=0, high=10, shape=(10, 10), dtype=np.uint8).sample()  # Returns a random array\n",
    "# We will use this for our frame data\n",
    "# In this case the shape would be passed as (640, 480) to fit the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ViZDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env):  # inherit from the Env base class\n",
    "    # Initialization method\n",
    "    def __init__(self, render=False, config=\"git_doom/ViZDoom/scenarios/deadly_corridor_1.cfg\"):  # Render false lets us train faster\n",
    "        # Inherit from env\n",
    "        super().__init__()\n",
    "        # Game setup\n",
    "        self.game = DoomGame()\n",
    "        # Config now loaded as param for curriculum learning\n",
    "        self.game.load_config(config)\n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        self.game.init()  # Start the game after setting the params\n",
    "\n",
    "        # Define the action and observation space\n",
    "        self.observation_space = Box(low=0, high=255, \n",
    "            shape=(100, 160, 1), dtype=np.uint8)  # Resized shapes\n",
    "        self.action_space = Discrete(7)  # Straightforward \n",
    "\n",
    "        # Game variables: HEALTH DAMAGE_TAKEN HITCOUNT SELECTED_WEAPON_AMMO\n",
    "        self.damage_taken = 0\n",
    "        self.hitcount = 0\n",
    "        self.selected_weapon_ammo = 52\n",
    "        # We want the CHANGE in these values\n",
    "\n",
    "    # Perform an action/take a step in the environment\n",
    "    def step(self, action):\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(7, dtype=np.uint8)\n",
    "        movement_reward = self.game.make_action(actions[action], 4)  # 4 frames skipped\n",
    "        reward = 0\n",
    "\n",
    "        # Get the other stuff we need to return, with error safety\n",
    "        if self.game.get_state():\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)\n",
    "\n",
    "            # Reward shaping\n",
    "            game_variables = self.game.get_state().game_variables\n",
    "            health, damage_taken, hitcount, ammo = game_variables\n",
    "\n",
    "            # Calculate delta (change)\n",
    "            damage_taken_delta = -damage_taken + self.damage_taken\n",
    "            self.damage_taken = damage_taken\n",
    "            hitcount_delta = hitcount - self.hitcount\n",
    "            self.hitcount = hitcount\n",
    "            ammo_delta = ammo - self.selected_weapon_ammo\n",
    "            self.selected_weapon_ammo = ammo\n",
    "\n",
    "            # Reward weights\n",
    "            reward = movement_reward + damage_taken_delta * 10 + hitcount_delta * 200 + ammo_delta * 5\n",
    "\n",
    "            info = ammo\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0  # Dummy descriptor\n",
    "        \n",
    "        info = {\"info\": info}\n",
    "        done = self.game.is_episode_finished()\n",
    "\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    # Gray Scale and resize the game frame with cv2\n",
    "    def grayscale(self, observation):  # Effectively passing our game frame\n",
    "        # The image is the same we're just moving the colour channels to the last index\n",
    "        # That's the format np expects\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_CUBIC)  # Make smaller\n",
    "        state = np.reshape(resize, (100, 160, 1))  # Reshape to fit our space\n",
    "        return state\n",
    "\n",
    "    # Start a new game\n",
    "    def reset(self):\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state) \n",
    "\n",
    "    # Close the environment/game\n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "    # Pre-Defined in VizDoom, staying pass for now\n",
    "    # Usually where we define how to render the game or environment\n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = VizDoomGym(render=True)\n",
    "\n",
    "# Testing\n",
    "state = env.reset()\n",
    "print(state.shape)\n",
    "\n",
    "print(f'Observation space shape: {env.observation_space.sample().shape}')\n",
    "print(f'Action space: {env.action_space.sample()}')\n",
    "\n",
    "print(env.step(2))\n",
    "\n",
    "plt.imshow(cv2.cvtColor(state, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Check if the environment is set up correctly with SB3's checker\n",
    "env_checker.check_env(env)  # Will throw error if not\n",
    "print(\"Environment is in correct format\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Training and Logging Callback, refer to SB3 docs\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "    \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, \"best_model_{}\".format(self.n_calls))\n",
    "            self.model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "CHECKPOINT_DIR_BASIC = './train/train_deadly'\n",
    "LOG_DIR_BASIC = './logs/log_deadly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, \n",
    "    save_path=CHECKPOINT_DIR_BASIC)\n",
    "# This saves the pytorch model's weights every 10000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm we're going to use to train our model is PPO\n",
    "# The PPO implementation can be found in Stable Baselines 3\n",
    "# Create a non rendered environment for training\n",
    "train_env = VizDoomGym(config=\"git_doom/ViZDoom/scenarios/deadly_corridor_1.cfg\")\n",
    "print(\"Creating training environment...\")\n",
    "\n",
    "# Create an instance of PPO\n",
    "model = PPO(\n",
    "    'CnnPolicy',  # Since we're using images\n",
    "    train_env,  # Our environment for training\n",
    "    tensorboard_log=LOG_DIR_BASIC,  # Where to save our logs\n",
    "    verbose=1,  # Verbosity level\n",
    "    learning_rate=0.00001,  # Learning rate\n",
    "    n_steps=8192,  # Number of steps to train\n",
    "    clip_range=0.1,  # Clip range 10% - clips the gradient to prevent significant change\n",
    "    gamma=0.95,  # Discount factor - 95%\n",
    "    gae_lambda=0.9  # Smoothing parameter to calculate the advantage, 90%\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.learn(\n",
    "    total_timesteps=400000,  # Number of steps to train\n",
    "    callback=callback,  # Callback to save the model\n",
    ")\n",
    "train_env.close()  # Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curriculum learning step 2\n",
    "train_env_2 = VizDoomGym(config=\"git_doom/ViZDoom/scenarios/deadly_corridor_2.cfg\")\n",
    "model.set_env(train_env_2)\n",
    "model.learn(\n",
    "    total_timesteps=100000,  # Number of steps to train\n",
    "    callback=callback,  # Callback to save the model\n",
    ")\n",
    "train_env_2.close()  # Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curriculum learning step 3\n",
    "train_env_3 = VizDoomGym(config=\"git_doom/ViZDoom/scenarios/deadly_corridor_3.cfg\")\n",
    "model.set_env(train_env_3)\n",
    "model.learn(\n",
    "    total_timesteps=100000,  # Number of steps to train\n",
    "    callback=callback,  # Callback to save the model\n",
    ")\n",
    "train_env_3.close()  # Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curriculum learning step 4\n",
    "train_env_4 = VizDoomGym(config=\"git_doom/ViZDoom/scenarios/deadly_corridor_4.cfg\")\n",
    "model.set_env(train_env_4)\n",
    "model.learn(\n",
    "    total_timesteps=100000,  # Number of steps to train\n",
    "    callback=callback,  # Callback to save the model\n",
    ")\n",
    "train_env_4.close()  # Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curriculum learning step 5\n",
    "train_env_5 = VizDoomGym(config=\"git_doom/ViZDoom/scenarios/deadly_corridor_5.cfg\")\n",
    "model.set_env(train_env_5)\n",
    "model.learn(\n",
    "    total_timesteps=100000,  # Number of steps to train\n",
    "    callback=callback,  # Callback to save the model\n",
    ")\n",
    "train_env_5.close()  # Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we test the model, fully trained\n",
    "\n",
    "# Load the model's most recent and best version\n",
    "model = PPO.load(\"./train/train_deadly/best_model_610000\")  # Can also use model.load\n",
    "\n",
    "# Test and evaluate model stats\n",
    "\n",
    "# Creat a test environment\n",
    "test_env = VizDoomGym(render=True, config=\"git_doom/ViZDoom/scenarios/deadly_corridor_3.cfg\")\n",
    "# Wrap with SB3 Monitor class for error safety\n",
    "test_env = Monitor(test_env, allow_early_resets=False)  # I don't want logs for this, you may set it to a log dir\n",
    "# Evaluate mean reward for 100 games\n",
    "print(\"Ripping and Tearing...\")\n",
    "mean_reward, _ = evaluate_policy(model, test_env, n_eval_episodes=10)\n",
    "print(\"It is done\")\n",
    "print(f\"Mean reward: {mean_reward}\")\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a loop to slow things down for visual clarity without evaluate_policy\n",
    "test_env = VizDoomGym(render=True, config=\"git_doom/ViZDoom/scenarios/deadly_corridor_3.cfg\")\n",
    "test_env = Monitor(test_env, allow_early_resets=False)\n",
    "\n",
    "for episode in range(5):  # Play 5 games\n",
    "    obs = test_env.reset()  # Store the current frame in observation\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)  # Get the action based on the observation\n",
    "        obs, reward, done, info = test_env.step(action)  # Get info from step\n",
    "        time.sleep(0.06)  # Have a small pause\n",
    "        total_reward += reward  # Update the reward\n",
    "    print(f\"Episode {episode} reward: {total_reward}\")\n",
    "    time.sleep(2)  # Between each episode\n",
    "\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
